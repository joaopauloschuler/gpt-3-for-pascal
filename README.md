# gpt-3-for-pascal
The paper [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) shows a neural network architecture based on a stack of transformer decoder modules.


